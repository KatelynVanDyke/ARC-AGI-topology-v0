{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30a3f822f4a1077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:12:28.773609Z",
     "start_time": "2025-12-08T00:12:27.107693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries and models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574a5efa7c65cb28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:12:28.852923Z",
     "start_time": "2025-12-08T00:12:28.789102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 training tasks.\n"
     ]
    }
   ],
   "source": [
    "# Load parsed training data\n",
    "with open(\"outputs/parsed_training.pkl\", \"rb\") as f:\n",
    "    training = pickle.load(f)\n",
    "\n",
    "print(\"Loaded\", len(training), \"training tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ee4991f0988ec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:12:28.866728Z",
     "start_time": "2025-12-08T00:12:28.860519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions for grid processing\n",
    "def pad_grid(grid, target_size=30):\n",
    "    H, W = grid.shape\n",
    "    out = -1 * np.ones((target_size, target_size), dtype=int)\n",
    "    out[:H, :W] = grid\n",
    "    return out\n",
    "\n",
    "def grid_to_vector(grid):\n",
    "    return pad_grid(grid, target_size=30).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41474994347291c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:12:28.970883Z",
     "start_time": "2025-12-08T00:12:28.897841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw-grid dataset: (3232, 900) (3232,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare raw grid dataset\n",
    "X_raw = []\n",
    "y_raw = []\n",
    "\n",
    "for task_id, task in training.items():\n",
    "    for inp, out in zip(task[\"train_inputs\"], task[\"train_outputs\"]):\n",
    "        X_raw.append(grid_to_vector(inp))\n",
    "        y_raw.append(int(out.sum()) % 10)\n",
    "\n",
    "X_raw = np.vstack(X_raw)\n",
    "y_raw = np.array(y_raw)\n",
    "\n",
    "print(\"Raw-grid dataset:\", X_raw.shape, y_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a740fbe633dcee36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:12:29.045247Z",
     "start_time": "2025-12-08T00:12:28.977906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load feature data\n",
    "df_features = pd.read_pickle(\"outputs/features_training.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e42025102271846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:12:29.387477Z",
     "start_time": "2025-12-08T00:12:29.053653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-based dataset: (6464, 18) (6464,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature-based dataset\n",
    "df_features = pd.read_pickle(\"outputs/features_training.pkl\")\n",
    "\n",
    "group_cols = [\"task_id\", \"grid_role\", \"grid_index\"]\n",
    "\n",
    "numeric_cols = df_features.select_dtypes(include=[int, float]).columns\n",
    "numeric_cols = [c for c in numeric_cols if c not in group_cols]\n",
    "\n",
    "df_grouped = (\n",
    "    df_features\n",
    "    .groupby(group_cols)[numeric_cols]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "mask = df_grouped[\"grid_role\"].isin([\"train_input\", \"train_output\"])\n",
    "df_train_grids = df_grouped[mask].copy()\n",
    "\n",
    "X_feat = df_train_grids.drop(columns=group_cols).to_numpy()\n",
    "\n",
    "y_feat = []\n",
    "\n",
    "for _, row in df_train_grids.iterrows():\n",
    "\n",
    "    tid  = row[\"task_id\"]\n",
    "    idx  = int(row[\"grid_index\"])\n",
    "    out_grid = training[tid][\"train_outputs\"][idx]\n",
    "    y_feat.append(int(out_grid.sum()) % 10)\n",
    "\n",
    "y_feat = np.array(y_feat)\n",
    "\n",
    "print(\"Feature-based dataset:\", X_feat.shape, y_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "235ad9aa4560c97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:12:29.400984Z",
     "start_time": "2025-12-08T00:12:29.394411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define machine learning models\n",
    "max_iter = 5000\n",
    "n_estimators = 500\n",
    "\n",
    "models = {\n",
    "    # Baselines\n",
    "    \"LogReg\": LogisticRegression(max_iter=max_iter),\n",
    "    \"LinearSVM\": LinearSVC(max_iter=max_iter),\n",
    "    \"SGD-Hinge\": SGDClassifier(loss=\"hinge\", max_iter=max_iter, tol=1e-3),\n",
    "\n",
    "    # Kernel methods\n",
    "    \"RBF-SVM\": SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=False, max_iter=max_iter),\n",
    "\n",
    "    # Bagged / Randomized trees\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=n_estimators),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=n_estimators),\n",
    "\n",
    "    # Boosted decision trees\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=n_estimators),\n",
    "\n",
    "    # Gradient boosting (modern variant)\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"logloss\"\n",
    "    ),\n",
    "\n",
    "    # Neural networks\n",
    "    \"NeuralNet\": MLPClassifier(\n",
    "        hidden_layer_sizes=(128,),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        max_iter=max_iter,\n",
    "        random_state=0\n",
    "    ),\n",
    "    \"NeuralNet-Deep\": MLPClassifier(\n",
    "        hidden_layer_sizes=(256, 128),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        max_iter=max_iter,\n",
    "        random_state=0\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b9a1d8eab1d471",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:49:52.880694Z",
     "start_time": "2025-12-08T00:12:29.411447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RAW model: LogReg\n",
      "Training RAW model: LinearSVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvand\\PycharmProjects\\ARC\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RAW model: SGD-Hinge\n",
      "Training RAW model: RBF-SVM\n",
      "Training RAW model: RandomForest\n",
      "Training RAW model: ExtraTrees\n",
      "Training RAW model: GradientBoosting\n",
      "Training RAW model: AdaBoost\n",
      "Training RAW model: XGBoost\n",
      "Training RAW model: NeuralNet\n",
      "Training RAW model: NeuralNet-Deep\n"
     ]
    }
   ],
   "source": [
    "# Train models on raw grids\n",
    "Path(\"outputs/models\").mkdir(exist_ok=True)\n",
    "\n",
    "for name, model in models.items():\n",
    "\n",
    "    print(f\"Training RAW model: {name}\")\n",
    "    model.fit(X_raw, y_raw)\n",
    "\n",
    "    with open(f\"outputs/models/raw_{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ae3de76c3034ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T00:50:45.338329Z",
     "start_time": "2025-12-08T00:49:52.899720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FEATURE model: LogReg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvand\\PycharmProjects\\ARC\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 5000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=5000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FEATURE model: LinearSVM\n",
      "Training FEATURE model: SGD-Hinge\n",
      "Training FEATURE model: RBF-SVM\n",
      "Training FEATURE model: RandomForest\n",
      "Training FEATURE model: ExtraTrees\n",
      "Training FEATURE model: GradientBoosting\n",
      "Training FEATURE model: AdaBoost\n",
      "Training FEATURE model: XGBoost\n",
      "Training FEATURE model: NeuralNet\n",
      "Training FEATURE model: NeuralNet-Deep\n"
     ]
    }
   ],
   "source": [
    "# Train models on features\n",
    "for name, model in models.items():\n",
    "\n",
    "    print(f\"Training FEATURE model: {name}\")\n",
    "    model.fit(X_feat, y_feat)\n",
    "\n",
    "    with open(f\"outputs/models/feat_{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
